La complejidad temporal y espacial del algoritmo de recocido simulado modificado se puede analizar considerando las operaciones principales que realiza y las estructuras de datos que utiliza.

Complejidad Temporal
El algoritmo de recocido simulado tiene varias fases en las que se deben considerar los costos en términos de tiempo:

Generación de Partición Inicial:

generar_particion_aleatoria tiene una complejidad de 
�
(
�
)
O(n) donde 
�
n es el número de estados presentes. Esto es porque la función itera sobre todos los elementos de estados_presentes para crear las particiones iniciales.
Iteraciones del Recocido Simulado:

El algoritmo itera hasta que la temperatura final es alcanzada. El número de iteraciones es controlado por el factor de enfriamiento, que decrece la temperatura geométricamente. Si consideramos 
�
0
T 
0
​
  como la temperatura inicial y 
�
�
T 
f
​
  como la temperatura final, el número de iteraciones necesarias es aproximadamente 
�
(
log
⁡
�
�
�
�
�
�
_
�
�
�
�
�
�
�
�
�
�
�
�
(
�
0
/
�
�
)
)
O(log 
factor_enfriamiento
​
 (T 
0
​
 /T 
f
​
 )).
Iteraciones por Temperatura:

En cada temperatura, el algoritmo realiza un número fijo de iteraciones iteraciones_por_temperatura, que es 
�
(
�
)
O(n).
Generación de Vecino y Evaluación de Costos:

generar_vecino y calcular_costo ambos tienen una complejidad de 
�
(
�
)
O(n), ya que pueden involucrar la manipulación y evaluación de todos los estados presentes.
Resumen de Complejidad Temporal
La complejidad total temporal 
�
(
�
)
T(n) del algoritmo puede aproximarse combinando estos factores:

�
(
�
)
=
�
(
log
⁡
�
�
�
�
�
�
_
�
�
�
�
�
�
�
�
�
�
�
�
(
�
0
�
�
)
⋅
�
⋅
�
)
=
�
(
�
2
log
⁡
�
�
�
�
�
�
_
�
�
�
�
�
�
�
�
�
�
�
�
(
�
0
�
�
)
)
T(n)=O(log 
factor_enfriamiento
​
 ( 
T 
f
​
 
T 
0
​
 
​
 )⋅n⋅n)=O(n 
2
 log 
factor_enfriamiento
​
 ( 
T 
f
​
 
T 
0
​
 
​
 ))

Complejidad Espacial
La complejidad espacial del algoritmo depende principalmente de las estructuras de datos que mantiene en memoria:

Particiones:

El algoritmo mantiene varias particiones, cada una de tamaño 
�
(
�
)
O(n), ya que las particiones contienen listas de estados presentes y futuros.
Costos:

Mantiene costos asociados con cada partición, que son valores escalar.
Conjuntos de Particiones Visitadas:

Utiliza un conjunto para almacenar particiones visitadas. En el peor de los casos, este conjunto puede crecer proporcionalmente al número de particiones generadas. Sin embargo, dado que estamos en un entorno probabilístico, no todas las particiones posibles se almacenan.
Resumen de Complejidad Espacial
La complejidad espacial 
�
(
�
)
S(n) del algoritmo es principalmente lineal en función del número de estados:

�
(
�
)
=
�
(
�
)
S(n)=O(n)

Justificación
Generación y Evaluación:

La generación de particiones y vecinos es lineal porque se basa en la manipulación directa de listas de tamaño 
�
n.
Iteraciones Controladas por Temperatura:

La estructura de control por temperatura asegura que el algoritmo no necesita explorar todas las posibles particiones exhaustivamente, limitando la explosión combinatoria.
Uso de Conjuntos:

El uso de conjuntos para almacenar particiones visitadas ayuda a evitar ciclos y repetición, pero también es manejable en términos de memoria ya que el número de iteraciones y vecinos generados está controlado.
Conclusión
El algoritmo tiene una complejidad temporal de 
�
(
�
2
log
⁡
�
�
�
�
�
�
_
�
�
�
�
�
�
�
�
�
�
�
�
(
�
0
�
�
)
)
O(n2logfactor_enfriamiento(TfT0​)) y una complejidad espacial de 
�
(
�
)
O(n). Esta eficiencia es adecuada para problemas de optimización donde encontrar la mejor partición exacta sería demasiado costoso y se prefiere una buena solución en un tiempo razonable.







def generar_vecino(particion):
    ep1, ef1, vp1, ep2, ef2, vp2 = particion

    if len(ep1) > 0 and len(ep2) > 0:
        i1 = random.randint(0, len(ep1) - 1)
        i2 = random.randint(0, len(ep2) - 1)
        ep1[i1], ep2[i2] = ep2[i2], ep1[i1]
        vp1[i1], vp2[i2] = vp2[i2], vp1[i1]

    if len(ef1) > 0 and len(ef2) > 0:
        i1 = random.randint(0, len(ef1) - 1)
        i2 = random.randint(0, len(ef2) - 1)
        ef1[i1], ef2[i2] = ef2[i2], ef1[i1]

    if not ep1 and not ef1:
        ef1.append(ef2.pop())
    if not ep2 and not ef2:
        ef2.append(ef1.pop())

    return (ep1, ef1, vp1, ep2, ef2, vp2)


    n = len(estados_presentes)
    m = len(estados_futuros)
    indices = list(range(n))
    random.shuffle(indices)
    mitad = n // 2

    ep1_indices = indices[:mitad]
    ep2_indices = indices[mitad:]

    ep1 = [estados_presentes[i] for i in ep1_indices]
    vp1 = [valores_estados_presentes[i] for i in ep1_indices]
    ep2 = [estados_presentes[i] for i in ep2_indices]
    vp2 = [valores_estados_presentes[i] for i in ep2_indices]

    # Asegurarse de que la muestra no sea mayor que el tamaño de la lista
    ef1 = random.sample(estados_futuros, random.randint(0, m))
    ef2 = [estado for estado in estados_futuros if estado not in ef1]

    if not ep1 and not ef1:
        ef1.append(ef2.pop())
    if not ep2 and not ef2:
        ef2.append(ef1.pop())

    return (ep1, ef1, vp1, ep2, ef2, vp2)